#   bbrot.py
#                   v2
#
#       2022.07.09

#ライブラリの読み込み
import math
from webbrowser import BackgroundBrowser
from cv2 import COLOR_GRAY2BGR, DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS
import numpy as np
import cv2
import os
import matplotlib.pyplot as plt
from PIL import Image
import sys
sys.path.append('/path/to/dir')
import pyocr
import pyocr.builders

#ウインドウの設定
cv2.namedWindow('template3', cv2.WINDOW_NORMAL)
cv2.moveWindow('template3', 0, 200)
cv2.namedWindow('match BB', cv2.WINDOW_NORMAL)
cv2.moveWindow('match BB', 200, 200)

#色の定義
mazenta = (255, 0, 255)
yellow  = (0, 255, 255)
green   = (0, 255, 0)
darkGreen = (0, 127, 0)
white   = (255, 255, 255)


def process(filename):
    """
    写っているすべてのBB弾の角度を推定

    Parameters
    ----------
    filename : string
        画像ファイル名

    Returns
    -------
        結果をオーバーレイした画像
    """

    image = cv2.imread(filename)
    #画像サイズと推定BB弾寸法（撮影範囲による）
    #撮影対象の大きさ[mm]
    subjectWidthMin = 210   #横方向撮影範囲寸法
    subjectWidthMax = 320
    bbObjectSize = 6        #BB弾の物理寸法
    #BB弾の推定サイズ[pixel]の計算
    bbPixelMin = int(image.shape[1] / subjectWidthMax * bbObjectSize)
    bbPixelMax = int(image.shape[1] / subjectWidthMin * bbObjectSize)
    print('画像サイズ (x:', image.shape[1], 'y:', image.shape[0], ')')
    print('検出するBB弾の大きさ', bbPixelMin, '~', bbPixelMax, 'pix')


    #グレースケール化(OCRで使用)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    #高さを中央付近450pxにクロップ
    w = image.shape[1]
    h = 450     #切り取り画像の縦ピクセル数
    top = (image.shape[0] - h) // 2
    flip = image[top: top + h, 0: w]
    #右から撃っているので、左から右への時系列になるように左右反転
    flip = cv2.flip(flip, 1)                                        # =0:上下反転、>0:左右反転、 <0:上下左右反転
    #コントラスト調整 (テンプレートマッチングで使用)
    scaled = cv2.convertScaleAbs(flip, alpha = 3.5, beta = 0)       #alpha:スケールファクタ1.0〜2.0 beta:加算値
    #ノイズ除去してブロブ検出用の画像を作成
    median = cv2.medianBlur(flip, ksize = 9)                        #ksizeは奇数 円の検出大きさに関わってくる
    #平坦化
    clahe = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8,8))
    cl1 = clahe.apply(flip)
    #二値化の閾値
    minGray = 0
    maxGray = 255
    #大津の手法　minGrayに大津の閾値が入る
    minGray, threshold = cv2.threshold(median, minGray, maxGray, cv2.THRESH_BINARY | cv2.THRESH_OTSU)

    hw = 350    #表示座標y
    hwd = 140
    winShow(flip, 'flip', (700, hw), (w // 4, h // 4))  #1/4に縮小
    winShow(scaled, 'scaled', (700, hw + hwd), (w // 4, h // 4))
    winShow(median, 'median', (700, hw + hwd + hwd), (w // 4, h // 4))
    winShow(threshold, 'threshold', (700, hw + hwd + hwd + hwd), (w // 4, h // 4))
    winShow(cl1, 'CLAHE', (700, hw + hwd + hwd + hwd + hwd), (w // 4, h // 4))
    print('ブロブ検出用二値化　大津の閾値 = ',minGray)

    #ヒストグラム
    hist1 = cv2.calcHist(flip, channels = [0], mask = None, histSize = [256], ranges = [0,256])
    hist2 = cv2.calcHist(scaled, channels = [0], mask = None, histSize = [256], ranges = [0,256])
    hist3 = cv2.calcHist(cl1, channels = [0], mask = None, histSize = [256], ranges = [0,256])
    '''
    plt.plot(hist1)  #ヒストグラム
    plt.plot(hist2)  #ヒストグラム
    plt.plot(hist3)  #ヒストグラム
    plt.show()
    '''
  
 
    ######################### 検出方法を選択 #####################################################
    flipBGR = cv2.cvtColor(flip.copy(), cv2.COLOR_GRAY2BGR)   #イメージ出力の元画像

    #detectMethod = "Blobs"
    detectMethod = "Hough"
    if detectMethod == 'Blobs':
        #Blob円検出による
        bbImg, bbData = circlesBlobs(flipBGR, threshold, minGray, maxGray, bbPixelMin, bbPixelMax) #　ブロブ円検出
    else:
        #Hough円検出による
        bbImg, bbData = circlesHough(flipBGR, median, bbPixelMin, bbPixelMax) #　ハフ円検出

    bbCount = len(bbData)
    winShow(bbImg, filename + detectMethod, (700, 190), (w // 4, h // 4))  #1/4に縮小

    cv2.waitKey(1)

#途中が抜けた時の処理ーーー未定


    #BB円平均サイズ(半径で計算、表示は直径)
    sumSize = 0
    for b in bbData:
        sumSize += b[3]
    bbAve = sumSize / bbCount    
    print('BBサイズ 平均直径 = {:4.1f}px'.format(bbAve * 2))

    bbMin = 0.97 * bbAve
    bbMax = 1.05 * bbAve
    print('BBサイズ 適正範囲 = {:4.1f}~{:4.1f}px'.format(bbMin * 2, bbMax * 2))
    bbData2 = []
    index = 0
    for b in bbData:
        #小さすぎるものを除外する
        if (b[3] > bbMax) or (b[3] < bbMin):
            print('No.', index, 'はサイズ不適合')
        else:
            #合格のデータだけで再構成
            bbData2.append(b)
        index += 1
    #print(bbData2)
    bbData = bbData2
    #index番号付け直し
    index = 0
    for b in bbData:
        b[0] = index
        index += 1
    bbCount = len(bbData)
    #print(bbData)
    print('適正BB検出数', bbCount)

    #BB検出数の判定
    if bbCount < 10:
        print('BB検出数が不足   any key to next')
        cv2.waitKey(0)
        return bbImg    #円検出だけの画像


    #BB進行軸線の傾斜角度
    dY = bbData[bbCount - 1][2] - bbData[0][2]
    dX = bbData[bbCount - 1][1] - bbData[0][1]
    incAngle = -math.degrees(math.atan(dY / dX))   #下向きをマイナス表示
    print('BBデータ [n,x,y,r]: ', bbData)
    print('傾斜角度: {:6.3f}/{:6.1f} = {:6.3f}deg'.format(dY, dX, incAngle))

    


    #BB弾検出結果から作業用画像を再度正確にクロップする
    bbImg = cv2.cvtColor(flip.copy(), cv2.COLOR_GRAY2BGR)   #イメージ出力の元画像　再作成
    #x座標
    offsetX = 250
    left = bbData[0][1] - offsetX
    if left < 0:
        left = 0
    right = bbData[bbCount - 1][1] + offsetX
    if right > bbImg.shape[1]:              #(y_size, x_size, color)
        right = bbImg.shape[1]
    #y座標
    offsetY = 85
    bby = sorted(bbData, key = lambda x: x[1])    #y座標でソート
    top = bby[0][2] - bbData[0][3] - offsetY
    if top < 0:
        top = 0
    bottom = bby[bbCount - 1][2]  + bbData[0][3] + offsetY  #(y_size, x_size, color)
    if bottom > bbImg.shape[0]:     
        bottom = bbImg.shape[0]

    #ブロブ検出＆作業書き込み用画像をクロップ
    d = len(bbImg.shape)
    if d <= 2:  #gray or color
        bbImg =  bbImg[top:bottom, left:right]
    else:
        bbImg = bbImg[top:bottom, left:right, :]
    #マッチ検出用画像もクロップ
    d = len(scaled.shape)
    if d <= 2:  #gray or color
        scaled =  scaled[top:bottom, left:right]
    else:
        scaled = scaled[top:bottom, left:right, :]
    #median画像もクロップ
    d = len(median.shape)
    if d <= 2:  #gray or color
        median =  median[top:bottom, left:right]
    else:
        median = median[top:bottom, left:right, :]

    #クロップ後の座標修正
    #bbcenter2 = []  #クリップ後の座標
    for i in range(bbCount):
        #bbcenter2.append((b[1] - left, b[2] - top))
        bbData[i][1] -= left
        bbData[i][2] -= top
    print(bbData)

    for b in bbData:
        # 円周を描画する
        cv2.circle(bbImg, (b[1], b[2]), b[3], green, thickness = 1)
        # 中心点を描画する
        cv2.drawMarker(bbImg, (b[1], b[2]), darkGreen, markerType = cv2.MARKER_CROSS, markerSize = 300, thickness = 1)

    cv2.waitKey(1)



    #### OCR #####   
    ocrLcd(image)
    #周期の入力
    while(1):
        inpt = input('1コマの周期 usec = ')
        if inpt == '':
            continue
        dt = float(inpt)
        if dt > 0:
         break


    #画像へコマ周期、初速、ファイル名を書き込み
    v0 = 0.012 / (dt / 1000000) 
    text = "{} circles:{:2d}  dt:{:5.1f}usec  v0:{:5.1f}m/sec  incline:{:6.3f}deg  ({})".format(detectMethod, bbCount, dt, v0, incAngle, filename)
    locate = (int(bbData[0][1]), int(bbData[0][2] + 100))
    cv2.putText(bbImg, text, locate, cv2.FONT_HERSHEY_SIMPLEX, 0.8, white, 1)



    cv2.imshow("BB image", bbImg) 
    cv2.imshow('scaled', scaled)
    cv2.imshow('median', median)
    cv2.waitKey(1)





############################################

    #カメラ撮影光軸中心付近のBB弾を基準とする＝テンプレート
    templateIndex = bbCount // 2   #//の答えは整数となる 
    tpCenter = (bbData[templateIndex][1], bbData[templateIndex][2])     #BB中心
    bbDia = bbData[templateIndex][3] * 2                                #BB直径

    tpDia = int(bbDia * 0.93)     #周辺の影の部分をマスク
    imgc = crop(scaled, tpCenter, (tpDia, tpDia))
    #imgc = crop(median, tpCenter, (tpDia, tpDia))  ###特徴点検出のとき####################################
    template = mask_circle(imgc, tpDia)
    #2値化オプション??###
    #ret, template = cv2.threshold(template, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU) 

#########特徴点検出用テンプレート（周辺画像までいれて広くしないと特徴点が検出されない）
    template2 = crop(median, tpCenter,(tpDia * 3, tpDia * 3))
#####################特徴点検出法


    #検出したBB弾すべてに角度推定を実行する
    print('計算開始')

    result = []
    anglefirst = 999
    for i in range(bbCount):
        point = (bbData[i][1], bbData[i][2])    #BB中心
        #回転角度を解析
        angle = estimate_rot(scaled, template, point, bbDia) ########scaled

#回転角がひっくり返っている時の処理　ーーー未定



########################################################
        #特徴点検出法のとき
        #angle2 = contourMatch(median, template2, point, bbData[templateIndex][3])
#########################################################


        #回転角度の線を表示
        lineLen = 160 // 2
        rad = math.radians(angle)
        dl = (math.cos(rad) * lineLen, -math.sin(rad) * lineLen)
        pt1 = np.add     (point, dl).astype(np.int16)
        pt2 = np.subtract(point, dl).astype(np.int16)
        cv2.line(bbImg, pt1, pt2, mazenta, thickness = 1)
        
        if anglefirst == 999:
            anglefirst = angle
            angleBefore = angle

        #角度の書き込み
        da = angle - angleBefore
        angleTotal = angle - anglefirst
        angleBefore = angle

        textAngle = '{:6.1f}deg'.format(angle)
        cv2.putText(bbImg, textAngle, org = (point[0] + 30, point[1] - 60), fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.8, color = mazenta, thickness = 1)
        if i >= 1:
            textDa = '({:5.1f})'.format(da)
            cv2.putText(bbImg, textDa, org = (point[0] + 65, point[1] - 30), fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.8, color = yellow, thickness = 1)

        #回転角の書き込み
        if i > templateIndex:
            try:
                kaiten = 1000000.0 / dt /(i * 360 / angleTotal)
                textRps =  '{:6.1f}rps'.format (kaiten)
                cv2.putText(bbImg, textRps, org = (point[0] + 0, point[1] + 90), fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.8, color = white, thickness = 1)
                #中心振り分けでの回転数計算
                dk = i - templateIndex
                kaiten2 = 1000000.0 / dt /((dk * 2) * 360 /(angle - result[templateIndex - dk][4]))
                textRps2 =  '{:6.1f}rps'.format (kaiten2)
                cv2.putText(bbImg, textRps2, org = (point[0] + 10, point[1] + 108), fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.6, color = yellow, thickness = 1)

            except ZeroDivisionError:
                kaiten = 0
        result.append((i, point[0], point[1], bbData[i][3], angle, da, angleTotal))

        cv2.imshow("BB image", bbImg) 
#################
        cv2.waitKey(300) ##################### 0: WAIT ###############
##################

    # 結果の表示など
    for i, d in enumerate(result):
        print(*d, sep = ',')
    
    # コマ数と回転角
    for r in result[templateIndex: ]:
        koma = r[0]
        try:
            kaiten = 1000000.0 / dt /(r[0] * 360 / r[6])
        except ZeroDivisionError:
            kaiten = 0    
        print('{:2d}コマ {:6.1f}rps'.format(koma, kaiten))
    return bbImg



def circlesHough(image, median, bbPixelMin, bbPixelMax):
    """
    ホフ円でBB弾を検出

    Parameters
    ----------
    image : mat
        出力画像の元
    median : mat
        検出する画像
    
    Returns
    -------
        円検出画像
        円位置座標とサイズ
    """
    #　ハフ円検出
    #円検出
    circles = cv2.HoughCircles(median, cv2.HOUGH_GRADIENT, dp = 1.0, minDist = 200, param1 = 100, param2 = 40, minRadius = bbPixelMin // 2-10, maxRadius = bbPixelMax // 2+20)
    #型変換
    circles = np.uint16(np.around(circles))
    circles = np.squeeze(circles)       #[]をひとつ削除する（サイズが1の次元が全て削除される　(1,16,3)->(16,3)　　）

    #円を描写
    for c in circles:
        # 円周を描画する
        cv2.circle(image, (c[0], c[1]), c[2], green, thickness = 1)
        # 中心点を描画する
        cv2.drawMarker(image, (c[0], c[1]), darkGreen, markerType = cv2.MARKER_CROSS, markerSize = 300, thickness = 1)
    
    #x位置順にソート
    circles = sorted(circles, key=lambda x: x[0])    #x[0]:x座標
    #BBデータを整理
    bbData = []        #(n, x, y, r)
    bbnum = 0
    for c in circles:
        bbData.append([bbnum, c[0], c[1], c[2]])
        bbnum += 1

    #円の個数
    bbcount = len(bbData)
    print(f'Hough circles: {bbcount}')
    print(bbData)
    return image, bbData



def circlesBlobs(image, threshold, minGray, maxGray, bbPixelMin, bbPixelMax):
    """
    ブロブ円でBB弾を検出

    Parameters
    ----------
    image : mat
        出力画像の元
    threshold : mat
        検出する画像
    minGray : int
    maxGray : int
        二値化の閾値 0 ~ 255
    bbPixelMin : int
    bbPixelMax : int
        BB弾の大きさ px
    
    Returns
    -------
        円検出画像
        円位置座標とサイズ
    """
    #ブロブ円検出
    keypoints = blobsDetect(threshold, minGray, maxGray, bbPixelMin, bbPixelMax)
    #ブロブを円形で表示
    image = cv2.drawKeypoints(image, keypoints, None, green, cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)
    #x位置順にソート
    keypoints = sorted(keypoints, key=lambda kp: kp.pt[0])    #kp.pt[0]:x座標
    #BBデータを整理
    bbData = []        #(n, x, y, r)
    bbnum = 0
    for k in keypoints:
        point = (int(k.pt[0]), int(k.pt[1]))
        #ブロブの中心を表示
        cv2.drawMarker(image, point, darkGreen, markerType = cv2.MARKER_CROSS, markerSize = 300, thickness = 1)
        bbData.append([bbnum, point[0], point[1], int(k.size / 2)])
        bbnum += 1

    #ブロブの個数
    bbcount = len(bbData)
    print(f'blob circles: {bbcount}')   
    print(bbData)
    return  image, bbData


def blobsDetect(image, minGray, maxGray, bbPixelMin, bbPixelMax):
    """
    ブロブ（かたまり）検出器を設定して検出

    Parameters
    ----------
    image : mat
        検出対象の画像
    minGray : int
    maxGray : int
        二値化の閾値 0 ~ 255
    bbPixelMin : int
    bbPixelMax : int
        BB弾の大きさ px
    
    Returns
    -------
        ブロブ位置座標とサイズ
    """

    #ブロブ（塊）検出器の設定
    params = cv2.SimpleBlobDetector_Params()
    #白のブロブを検出
    params.blobColor = 255      #黒検出したい時は0
     #閾値
    params.minThreshold = minGray
    params.maxThreshold = maxGray
    #塊の大きさ（面積）
    params.filterByArea = True
    shade = 0.95      #影、インク模様の面積分の補正
    params.minArea = bbPixelMin ** 2 * math.pi / 4 * shade      ##小さい円が入りすぎ＃＃＃＃＃＃＃＃＃＃＃
    params.maxArea = bbPixelMax ** 2 * math.pi / 4
    #円形度でフィルタ(凹面concave)
    params.filterByCircularity = True
    params.minCircularity = 0.4   #0〜1 = 4πS/L^2   S:面積(画素数) L:周囲長　　　円形度が高い->1.0
    ###模様が外周にかかって輪郭が切れた時に凹面になるので小さめの値にする
    #凸面フィルタ
    params.filterByConvexity = True
    params.minConvexity = 0.5       #0〜1 = S/C  S:面積　C:凸面の面積（円形から出っぱった分）
    #楕円形フィルタ（形態の伸び　円形=1、直線=0 慣性モーメント）
    params.filterByInertia = True
    params.minInertiaRatio = 0.5
    #検出器を設定
    ver = (cv2.__version__).split('.')
    if int(ver[0]) <= 2:
        #openCV ver.2
        detector = cv2.SimpleBlobDetector(params)
    else:
        #openCV ver.3~
        detector = cv2.SimpleBlobDetector_create(params)  

    #検出器を作動（ブロブを検出する）
    keypoints = detector.detect(image)
    return keypoints


def estimate_rot(image, template, pt, size):
    """
    回転角の推定

    Parameters
    ----------
    image : mat
        推定対象の画像
    template : mat
        テンプレート画像
    pt : [number, number]
        推定対象の中心位置[col,row]
    size : any
        比較するBB画像の直径 px 

    Returns
    -------
    回転角 -180~+180(deg)
    """
    angle = -1
    max = 1
    ext = 10    #周りを少し広く
    cr = crop(image, pt, (size + ext, size + ext ))
    cv2.imshow("match BB", cr)
    (w, h) = template.shape[: : -1] 

    #### whole test #####
    '''
    img2 = image.copy()
    matchResult = cv2.matchTemplate(img2, template, cv2.TM_CCOEFF)
    _, maxVal, _, maxLoc = cv2.minMaxLoc(matchResult)   ###minVal,maxVal,minIndex,maxIndex
    topLeft = maxLoc
    bottomRight = (topLeft[0] + w, topLeft[1] + h)
    cv2.rectangle(img2, topLeft, bottomRight, (255, 0, 255), 3)
    #マッチリザルト
    plt.subplot(121),plt.imshow(matchResult, cmap = 'gray')
    plt.title('Matching Result'), plt.xticks([]), plt.yticks([])
    plt.subplot(122),plt.imshow(img2, cmap = 'gray')
    plt.title('Detected Point'), plt.xticks([]), plt.yticks([])
    plt.show()
    #うまくいかない
    '''
    #######################


    # ぐるぐる回しながら相関が最大となる角度を求める
    denomi = 1      #角度計算のステップ　1度/denomi  2->0.5deg 10->0.1deg
    matchAngle = 0
    for i in range(-180 * denomi, 180 * denomi):
        angle = i / denomi
        tp = rot(template, angle)
        matchResult = cv2.matchTemplate(cr, tp, cv2.TM_CCOEFF)
        _, maxVal, _, maxLoc = cv2.minMaxLoc(matchResult)   ###minVal,maxVal,minIndex,maxIndex
        
        if(maxVal > max):
            cr2 = cr.copy()
            cr2 = cv2.cvtColor(cr2, COLOR_GRAY2BGR)
            matchAngle = angle
            max = maxVal
            if denomi <= 2:
                #細かく計算する時は表示しない
                #角度表示
                cv2.putText(tp, str(matchAngle), org = (0, 15), fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.4, color = 0, thickness = 1)
                # org は左下の座標
                #マッチ値表示
                cv2.putText(tp, str(int(max)), org = (0, 100), fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.4, color = 0, thickness = 1)
                topLeft = maxLoc
                bottomRight = (topLeft[0] + w, topLeft[1] + h)
                cv2.rectangle(cr2, topLeft, bottomRight, (255, 0, 255), 1)
            
                cv2.imshow("match BB", cr2)
                cv2.imshow("template3", tp)
                cv2.waitKey(1)

                #マッチ　類似度のヒートマップ
                #fig, ax = plt.subplots(figsize=(10, 5))
                #im = ax.imshow(matchResult, cmap="jet")
                #plt.show()
            
    #cv2.waitKey(300)
    print(matchAngle , '° Match value:' ,max)
    return matchAngle



def contourMatch(image, template, pt, size):
    """
    特徴点検出にて角度を求める

    Parameters
    ----------
    image : mat
        BB弾列の画像
    template : mat
        テンプレート画像
    pt : [number, number]
        推定対象の中心位置[col,row]
    size : any
        比較するBB画像の直径 px 

    Returns
    -------
    回転角 -180~+180(deg)#####################
    """

    ext = 200    #周りを少し広く
    cr = crop(image, pt, (size + ext, size + ext ))
    #ret, template = cv2.threshold(template, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
    #ret, cr = cv2.threshold(cr, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)

    #cv2.imshow("match BB2", template)
    match(template, cr)
    return


def match(img1, img2):
    # 特徴点抽出
    det = cv2.ORB_create()#ORB
    # 各画像の特徴点を取る
    kp1, des1 = det.detectAndCompute(img1, None)
    kp2, des2 = det.detectAndCompute(img2, None)
    
    # 2つの特徴点をマッチさせる
    bf = cv2.BFMatcher(cv2.NORM_HAMMING)#, crossCheck = True)
    #matches = bf.match(des1, des2)
    matches = bf.knnMatch(des1, des2, k = 2)

    # レシオテストを行う
    th = 0.85 #0.60
    good = []
    for m, n in matches:
        if m.distance < n.distance * th:
            good.append([m])

    # 特徴点を同士をつなぐ
    img3 = cv2.drawMatchesKnn(img1, kp1, img2, kp2, good, None)
    #cv2.imshow('keypoint match', img3)
    #cv2.waitKey(0)

    #変数の形を合わせる　＝　配列の次元と数を揃える
    nu = len(good)
    fromKp = []
    for k in kp1[:nu]:
        fromKp.append([k.pt[0],k.pt[1]])
    fromKp = np.reshape(fromKp, (-1, 1, 2))

    toKp = []
    for k in kp2[:nu]:
        toKp.append([k.pt[0],k.pt[1]])
    toKp   = np.reshape(toKp, (-1, 1, 2))

    aA, inliers = cv2.estimateAffinePartial2D(fromKp, toKp)     #2つの変数の数が合わないとダメなよう

    #平行移動量
    mM = aA[:2, :2]
    t = aA[:, 2]
    #回転角度
    degree = np.rad2deg(-np.arctan2(aA[0, 1], aA[0, 0]))
    print('特徴点　回転角度＝' , degree)
    #print(' M', mM, 't', t)

    textDa = '{:6.2f}deg'.format(degree)
    cv2.putText(img3, textDa, org = (160, 250), fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 1.0, color = white, thickness = 1)

    cv2.imshow('keypoint match', img3)
    cv2.waitKey(1)


    return img3, (kp1, des1, kp2, des2, matches)



# 画像の表示
def display(img, output_file_path):
    cv2.imwrite(output_file_path, img)
    plt.imshow(plt.imread(output_file_path))
    plt.axis('off')
    plt.show()







def crop(image, pt, size):
    """
    画像の一部を矩形で切り取る
    
    Parameters
    ----------
    image : mat
        切り出し元の画像
    pt : [number, number]
        切り取り中心[col, row]
    size : [number, number]
        切り取りサイズ[width, height]

    Returns
    -------
        切り出した画像
    """
    left = int(pt[0] - size[0] / 2)
    if left < 0:
        left = 0      
    right = int(pt[0] + size[0] / 2)
    top = int(pt[1] - size[1] / 2)
    if top < 0:
        top = 0
    bottom = int(pt[1] + size[1] / 2)
    d = len(image.shape)
    if d <= 2:
        #grayscale
        return image[top:bottom, left:right]
    else:
        #color
        return image[top:bottom, left:right, :]



def ocrLcd(image):
    """
    OCRで発光周期時間を読み取り
    
    Parameters
    ----------
    image : mat
        画像
 
    Returns
    -------
        読み取った文字列
    """

    #lcdImage = crop(image, ( 1800, 800) , ( 1000, 1000)) 
    #lcdImage = cv2.convertScaleAbs(lcdImage, alpha = 0.5, beta = -20)
    #lcdImage = cv2.medianBlur(lcdImage, ksize = 1) #ksizeは奇数
    #ret, lcdImage = cv2.threshold(lcdImage, 95, 255,cv2.THRESH_BINARY)

    # 20200717 P110170~
    lcdImage = crop(image, ( 2300, 3000) , ( 1000, 1000))  
    lcdImage = cv2.convertScaleAbs(lcdImage, alpha = 0.5, beta = -20)
    lcdImage = cv2.medianBlur(lcdImage, ksize = 3) #ksizeは奇数
    ret, lcdImage2 = cv2.threshold(lcdImage, 0, 255,cv2.THRESH_BINARY | cv2.THRESH_OTSU)
    print('OCR用二値化　大津の閾値:', ret)
    ret,lcdImage = cv2.threshold(lcdImage, ret + 35, 255,cv2.THRESH_BINARY)
    #retはOTSUのときのしきい値
    kernel = np.ones((3,3),np.uint8)
    lcdImage = cv2.erode(lcdImage, kernel, 4)

    #lcdImage = rot(lcdImage, 91.5)
    lcdImage = rot(lcdImage, -90)
    #cv2.imshow("image4", lcdImage)
    #cv2.waitKey(1) 
    lcdImage = cv2.bitwise_not(lcdImage)    #反転
    #cv2.imshow("image4", lcdImage)
    #cv2.waitKey(1) 

    #####test 文字
    cv2.putText(lcdImage, "* test 12.3m/sec (456.7us) test *", org = (20, 860), fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 1.0, color = (0,0,0), thickness = 2)
    cv2.imwrite('ocrimg.png', lcdImage)

    cv2.namedWindow("lcdImage", cv2.WINDOW_NORMAL)
    h = lcdImage.shape[0]
    w = lcdImage.shape[1]
    rLcdImage = cv2.resize(lcdImage, (w // 2, h // 2))

    cv2.imshow("lcdImage", rLcdImage)
    cv2.moveWindow("lcdImage", 0, 500)

    #cv2.imwrite("test.png",lcdImage)
    cv2.waitKey(1) 
    
    tools = pyocr.get_available_tools()
    if len(tools) == 0:
        #print("No OCR tool found")
        SystemExit(1)

    tool = tools[0]
    #print("will use tool '%s'" % (tool.get_name()))
    langs = tool.get_available_languages()
    #print("available languages: %s" % ", ".join(langs))

    #builder = pyocr.builders.TextBuilder(tesseract_layout =  6) #text
    builder = pyocr.builders.WordBoxBuilder(tesseract_layout=6) #box
    txt = tool.image_to_string(Image.open('ocrimg.png'), lang = 'eng', builder = builder)
    
    out = cv2.imread('ocrimg.png')
    for r in txt:
        #print(r.content, end = ' ')
        #print(r.position)
        cv2.rectangle(out, r.position[0], r.position[1],(255, 0, 255), 2)

    cv2.imshow('lcdImage', out)
    cv2.waitKey(1)
    #print(txt)
    #print()

    return txt


def rot(image, degree):
    """
    画像を画像中央を中心に回転させる。回転により生じる背景は白で塗りつぶし。
    
    Parameters
    ----------
    image : mat
        元画像
    degree : float
        回転角度(deg)

    Returns
    -------
    回転した画像
    """
    (h, w) = (0, 0)
    if(len(image.shape)):
        #gray scale
        (h, w) = image.shape
        bg = 255
    else:
        #color
        (h, w) = image.shape[:2]
        bg = (255, 255, 255)
    center = (w / 2.0, h / 2.0)
    mat = cv2.getRotationMatrix2D(center, degree, scale = 1.0)  #回転のための変換行列を生成
    return cv2.warpAffine(image, mat, (w, h), borderValue = bg)     #アフィン変換


def mask_circle(image, dia):
    """
    円形でマスク。

    Parameters
    ----------
    image : mat
        画像
    dia : int
        円の直径(px)

    Returns
    -------
    マスクした画像
    """
    #マスク画像の元を作成（白一面）
    mask = np.full(image.shape[:2], 255, dtype = image.dtype)

    #白い画像の中心に黒い円を描画する。
    centerX = image.shape[1] // 2
    centerY = image.shape[0] // 2
    cv2.circle(mask, (centerX, centerY), (dia // 2), color = 0, thickness = - 1)
    #cv2.imshow('TMask', mask)
    #cv2.waitKey(0)
    return cv2.bitwise_or(image, mask)


def winShow(image, name, pt, size):
    """
    画像をウインドウで表示
    Parameters
    ----------
    image : mat
        画像データ
    name : string
        ウインドウ名
    pt : [number, number]
        ウインドウ表示位置[col, row]pixel
    size : [number, number]
        ウインドウに表示する画像サイズ[width, height]pixel
    Returns
    -------
    なし
    """
    cv2.namedWindow(name, cv2.WINDOW_NORMAL)
    cv2.imshow(name, image)
    cv2.moveWindow(name, pt[0], pt[1])
    cv2.resizeWindow(name, size[0], size[1])
    return



####  main  ######
#指定したフォルダの画像すべてに対して実行
#pythonの関数の定義位置は、実際に実行するまでに定義されていればよい。
#プログラムは先頭から実行されていくがdef関数定義の中身は実行されないので、実際に呼び出された時で考える。
#ここでprocess()が呼ばれるまでにprocess()中で呼ばれる関数は定義されているのでprocess()の後に定義でもオッケー

root = './bbpict/go'
files = os.listdir(path = root)
filesSorted = sorted(files)
filesSorted.remove('.DS_Store')
for f in filesSorted:
    openFileName = os.path.join(root, f)
    print()
    print()
    print('filename = ', openFileName)
    res = process(openFileName)
    savedFileName = "result" + f
    cv2.imwrite(savedFileName, res)

print('Complete')
print('any key -> exit system')
cv2.waitKey(0)


